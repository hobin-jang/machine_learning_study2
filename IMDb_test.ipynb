{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDb test.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1JSf2MXx5XWdWvu1/bMw9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hobin-jang/colab_test/blob/master/IMDb_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP4R7CLOc1qT"
      },
      "source": [
        "\"\"\"\r\n",
        "IMDb 영화 리뷰 데이터셋, 리뷰가 긍정인지 부정인지 판단\r\n",
        "\"\"\"\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJURZ8RBhk3K"
      },
      "source": [
        "# 데이터셋 : 미리 전처리되어 있음. 각 정수는 하나의 단어. 등장 빈도에 따라 인덱스 붙힘. 낮은 정수일수록 빈도 높음\r\n",
        "x_train[0][:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYyOXUjnh9V7"
      },
      "source": [
        "word_index = tf.keras.datasets.imdb.get_word_index()\r\n",
        "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnnUnPwti_pe"
      },
      "source": [
        "# <pad>, <sos>, <unk> : 인덱스 0,1,2, 각각 패딩, sos, 알 수 없음\r\n",
        "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\r\n",
        "  id_to_word[id_] = token\r\n",
        "\r\n",
        "print(\" \".join([id_to_word[id_] for id_ in x_train[0][:10]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNa9UXjVjK_d"
      },
      "source": [
        "import tensorflow_datasets as tfds\r\n",
        "\r\n",
        "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\r\n",
        "train_size = info.splits[\"train\"].num_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85q5o1dbkdzY"
      },
      "source": [
        "def preprocess(x_batch, y_batch):\r\n",
        "  x_batch = tf.strings.substr(x_batch, 0, 300)\r\n",
        "  x_batch = tf.strings.regex_replace(x_batch, b\"<br\\\\s*/?>\", b\" \")\r\n",
        "  x_batch = tf.strings.regex_replace(x_batch, b\"[^a-zA-Z]\", b\" \")\r\n",
        "  x_batch = tf.strings.split(x_batch)\r\n",
        "  return x_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkEPwHk5lLHz"
      },
      "source": [
        "from collections import Counter\r\n",
        "vocabulary = Counter()\r\n",
        "for x_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\r\n",
        "  for review in x_batch:\r\n",
        "    vocabulary.update(list(review.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjNV_vYvl-DQ"
      },
      "source": [
        "vocabulary.most_common()[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqgGLeg2mJmP"
      },
      "source": [
        "vocab_size = 10000\r\n",
        "truncated_vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YtA1yJ1mdTS"
      },
      "source": [
        "words = tf.constant(truncated_vocabulary)\r\n",
        "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\r\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\r\n",
        "num_oov_buckets = 1000\r\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieDn2Javm3_q"
      },
      "source": [
        "table.lookup(tf.constant([b\"This movie was faaaaantastic\".split()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_KPCKyOnBuh"
      },
      "source": [
        "def encode_words(x_batch, y_batch):\r\n",
        "  return table.lookup(x_batch), y_batch\r\n",
        "\r\n",
        "train_set = datasets[\"train\"].batch(32).map(preprocess)\r\n",
        "train_set = train_set.map(encode_words).prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5Szf_MpoNNS"
      },
      "source": [
        "embed_size = 128\r\n",
        "model = tf.keras.Sequential([\r\n",
        "              tf.keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, input_shape=[None]),\r\n",
        "              tf.keras.layers.GRU(128, return_sequences=True),\r\n",
        "              tf.keras.layers.GRU(128),\r\n",
        "              tf.keras.layers.Dense(1, activation=\"sigmoid\")\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCwtKJi1ouIO"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBjWkowDo2f6"
      },
      "source": [
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\r\n",
        "history = model.fit(train_set, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG5z3Kk3pEt4"
      },
      "source": [
        "plt.plot(history.history[\"accuracy\"], label=\"accuracy\")\r\n",
        "plt.plot(history.history[\"loss\"], label=\"loss\")\r\n",
        "plt.ylim(0,1)\r\n",
        "plt.legend(loc=\"lower left\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emPyOVPItgqI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}