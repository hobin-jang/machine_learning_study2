{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fake_shakespeare.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOEovjWZdTWnixh13jOG0/t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hobin-jang/colab_test/blob/master/fake_shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Xb9Blj2iIV-"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ_tZxqJiSvG"
      },
      "source": [
        "shakespeare_url = \"https://homl.info/shakespeare\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjqaA3YI8OIC"
      },
      "source": [
        "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\r\n",
        "with open(filepath) as f:\r\n",
        "  shakespeare_text = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA0TW-Xu8abI"
      },
      "source": [
        "\"\"\"\r\n",
        "모든 글자 정수로 인코딩\r\n",
        "keras의 Tokenizer 클래스 사용\r\n",
        "char_level = True : 단어 수준 대신 글자 수준 인코딩\r\n",
        "\"\"\"\r\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\r\n",
        "tokenizer.fit_on_texts(shakespeare_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X4waT9887ol"
      },
      "source": [
        "tokenizer.texts_to_sequences(\"first\") # 텍스트를 인코딩한 결과"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7pw5oz-9YEP"
      },
      "source": [
        "tokenizer.texts_to_sequences([\"first\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl56WDx89knn"
      },
      "source": [
        "max_id = len(tokenizer.word_index) # 고유 글자 개수\r\n",
        "dataset_size = tokenizer.document_count # 전체 글자 개수\r\n",
        "print(max_id, dataset_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFV3nCDZ95i2"
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JbXArzQ-FRj"
      },
      "source": [
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1 # 0부터 인코딩하기 위해"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aANj4iKw-SrN"
      },
      "source": [
        "encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX5LCdIi-T0M"
      },
      "source": [
        "# 훈련 데이터 : 전체의 90%, 검증, 테스트 데이터 : 나머지\r\n",
        "train_size = dataset_size * 90 // 100\r\n",
        "# 한 번에 한 글자 반환\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEqxnr70AFPX"
      },
      "source": [
        "\"\"\"\r\n",
        "전체 글자 : 백만 개 이상의 시퀀스 하나\r\n",
        "이를 직접 RNN 훈련 시키면 백만 개의 층이 있는 것과 같다.\r\n",
        "그러므로 슬라이싱하여 (window 메서드 이용) 텍스트 윈도우로 나누어 부분 문자열을 이용한 RNN을 진행\r\n",
        "(TBPTT)\r\n",
        "\"\"\"\r\n",
        "n_steps = 100\r\n",
        "window_length = n_steps + 1\r\n",
        "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\r\n",
        "# window 메서드는 기본적으로 원도우를 중복하지 않음\r\n",
        "# shift=1 : 한 칸씩 옆으로 움직임 0~100, 1~101, ... default=window_length\r\n",
        "# drop_remainder = True : 모든 윈도우에 동일한 글자 포함되도록 (여기서는 101개)\r\n",
        "# 지정하지 않으면 글자 수 점점 줄여나감 101 > 100 > 99 > ... > 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1etnvw_CjWG"
      },
      "source": [
        "\"\"\"\r\n",
        "window 메서드는 각각 하나의 데이터셋으로 표현되는 윈도우를 포함하는 데이터셋을 만든다. (중첩 데이터)\r\n",
        "훈련에는 중첩 데이터셋을 바로 사용할 수 없음 => flat_map 메서드를 이용해 플랫 데이터로 변환\r\n",
        "{{1,2},{3,4,5,6}}을 flat 시키면 {1,2,3,4,5,6}\r\n",
        "lambda ds: ds.batch(2) : 각 데이터셋에 적용할 변환 함수를 flat_map 메서드에 전달해야 함\r\n",
        "위 경우를 전달하면 텐서 2개를 가진 데이터셋으로 변환 \r\n",
        "{{1,2},{3,4,5,6}} => {[1,2],[3,4],[5,6]}\r\n",
        "\"\"\"\r\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\r\n",
        "batch_size = 32\r\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\r\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5UrBgY_NgV0"
      },
      "source": [
        "\"\"\"\r\n",
        "고유 글자 수 적으므로 원-핫 인코딩 사용\r\n",
        "\"\"\"\r\n",
        "dataset = dataset.map(lambda x_batch, y_batch: (tf.one_hot(x_batch, depth=max_id), y_batch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brfc9UKENwjt"
      },
      "source": [
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SStUAx19Nzzj"
      },
      "source": [
        "# 모델 만들고 훈련 시키기\r\n",
        "model = tf.keras.models.Sequential([\r\n",
        "              tf.keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\r\n",
        "                                  dropout=0.2, recurrent_dropout=0.2),\r\n",
        "              tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\r\n",
        "              tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(max_id, activation=\"softmax\"))\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLPUFDetO9Vy"
      },
      "source": [
        "\"\"\"\r\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\r\n",
        "history = model.fit(dataset, epochs=20)\r\n",
        "\"\"\"\r\n",
        "# 오래 걸려서 학습 중지"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vExWpc7YDVc"
      },
      "source": [
        "# 전처리 함수\r\n",
        "def preprocessing(texts):\r\n",
        "  x = np.array(tokenizer.texts_to_sequences(texts)) - 1\r\n",
        "  return tf.one_hot(x, max_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9zAuAG7YaJj"
      },
      "source": [
        "# 간단한 다음 글자 예측\r\n",
        "# 위에서 학습 안 시키면 이상한 글 나옴\r\n",
        "X_new = preprocessing([\"How are yo\"])\r\n",
        "Y_pred = model.predict_classes(X_new)\r\n",
        "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beVoQfqvYtYk"
      },
      "source": [
        "# 가짜 셰익스피어 텍스트 만들기\r\n",
        "# 초기 텍스트를 입력하고 모델이 가장 가능성 있는 글자 예측\r\n",
        "# 이 글자를 텍스트 끝에 추가하고 늘어난 텍스트를 모델에 전달\r\n",
        "# 이를 반복 (temperature가 0에 가까울 수록 높은 확률의 글자 택함)\r\n",
        "def next_char(text, temperature=1):\r\n",
        "  X_new = preprocessing([text])\r\n",
        "  y_proba = model.predict(X_new)[0,-1:,:]\r\n",
        "  rescaled_logits = tf.math.log(y_proba) / temperature\r\n",
        "  char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\r\n",
        "  return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhgUZGYqZwFk"
      },
      "source": [
        "def complete_text(text, n_char=50, temperature=1):\r\n",
        "  for _ in range(n_char):\r\n",
        "    text += next_char(text, temperature)\r\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP5AEgvpZ8uM"
      },
      "source": [
        "complete_text(\"t\", temperature=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WfXg31GaU8t"
      },
      "source": [
        "complete_text(\"w\", temperature=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVresc-kaZQk"
      },
      "source": [
        "complete_text(\"e\", temperature=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEUb1Vjtacuc"
      },
      "source": [
        "# 상태가 있는 RNN (장기 기억 저장)\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\r\n",
        "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\r\n",
        "dataset = dataset.flat_map(lambda window : window.batch(window_length))\r\n",
        "dataset = dataset.batch(1)\r\n",
        "dataset = dataset.map(lambda windows : (windows[:, :-1], windows[:, 1:]))\r\n",
        "dataset = dataset.map(lambda x_batch, y_batch: (tf.one_hot(x_batch, depth=max_id), y_batch))\r\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QJe7pIsarzC"
      },
      "source": [
        "batch_size = 32\r\n",
        "encoded_parts = np.array_split(encoded[:train_size], batch_size)\r\n",
        "datasets = []\r\n",
        "for encoded_part in encoded_parts:\r\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\r\n",
        "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\r\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\r\n",
        "    datasets.append(dataset)\r\n",
        "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\r\n",
        "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\r\n",
        "dataset = dataset.map(\r\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\r\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L_a-SXpYxIF"
      },
      "source": [
        "model = tf.keras.models.Sequential([\r\n",
        "              tf.keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2, \r\n",
        "                                  batch_input_shape=[batch_size, None, max_id]),\r\n",
        "              tf. keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2),\r\n",
        "              tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(max_id, activation=\"softmax\"))\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAoQHa6bZdqV"
      },
      "source": [
        "class ResetStatesCallback(tf.keras.callbacks.Callback):\r\n",
        "  def on_epoch_begin(self, epoch, logs):\r\n",
        "    self.model.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft4WZlklZ1ZO"
      },
      "source": [
        "\"\"\"\r\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\r\n",
        "steps_per_epoch = train_size // batch_size // n_steps\r\n",
        "model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=50, callbacks=[ResetStatesCallback()])\r\n",
        "\"\"\"\r\n",
        "# 오래 걸려서 학습 중단"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKMhGel9aKL_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}